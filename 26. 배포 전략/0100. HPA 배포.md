# Module: Horizontal Pod Autoscaler (HPA)

## 1. HPA (Horizontal Pod Autoscaler) 개요

**HPA(Horizontal Pod Autoscaler)**는 쿠버네티스 클러스터에서 워크로드의 리소스 사용률(CPU, Memory 등)이나 사용자 정의 메트릭에 따라 **파드(Pod)의 개수를 자동으로 조절(Scale Out/In)**하는 컨트롤러이다.

수동으로 `kubectl scale` 명령어를 사용하거나 배포 파일의 `replicas`를 수정하는 대신, HPA는 실시간 트래픽 변화에 맞춰 탄력적으로 애플리케이션을 운영하게 해준다. 이는 클라우드 비용 절감과 서비스 안정성을 동시에 확보하는 핵심 오토스케일링 기술이다.

### 1.1 수평적 확장 vs 수직적 확장

* **Horizontal Scaling (Scale Out):** 파드의 개수를 늘리는 것. HPA가 담당한다. (예: 서버 2대 -> 10대)
* **Vertical Scaling (Scale Up):** 파드 자체의 리소스(CPU/RAM) 할당량을 늘리는 것. VPA(Vertical Pod Autoscaler)가 담당한다. (예: 2 Core -> 4 Core)

---

## 2. 동작 원리 및 아키텍처

HPA는 `kube-controller-manager` 내부에 존재하는 루프(Loop)로 구현되어 있으며, 기본적으로 15초(기본값)마다 메트릭을 수집하여 스케일링 여부를 결정한다.

### 2.1 핵심 구성 요소

1. **Metrics Server:** 각 노드의 cAdvisor로부터 리소스 사용량 데이터를 수집하여 HPA 컨트롤러에 제공하는 필수 애드온이다. (설치되어 있지 않으면 HPA는 동작하지 않음)
2. **HPA Controller:** 주기적으로 Metrics Server API를 호출하여 현재 리소스 사용량을 조회하고, 목표값과 비교한다.
3. **Deployment/ReplicaSet:** HPA가 계산한 결과에 따라 `replicas` 필드 값을 수정하여 실제 파드 개수를 조정한다.

### 2.2 스케일링 계산 알고리즘 (Algorithm)

HPA는 다음 공식을 사용하여 필요한 파드 개수를 계산한다.

* **예시:**
* 현재 파드 개수: 2개
* 현재 CPU 사용률: 90%
* 목표 CPU 사용률: 50%
* 계산:  -> 올림 처리하여 **4개**로 스케일 아웃 결정.



---

## 3. 설정 방법 및 YAML 명세

HPA를 사용하기 위해서는 반드시 배포되는 파드에 **리소스 요청량(Requests)**이 설정되어 있어야 한다. HPA는 이 `requests` 값을 기준으로 사용률(%)을 계산하기 때문이다.

### 3.1 필수 선행 조건: Pod Resource Request 설정

```yaml
# deployment.yaml
spec:
  containers:
  - name: my-app
    resources:
      requests:
        cpu: "200m"  # 기준값 (필수)
        memory: "256Mi"

```

### 3.2 HPA 설정 (v2beta2 / v2)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2   # 최소 파드 수 (트래픽이 없어도 유지)
  maxReplicas: 10  # 최대 파드 수 (비용 폭탄 방지)
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # CPU 사용률이 50%를 넘으면 스케일 아웃

```

---

## 4. 고급 메트릭 및 동작 제어

기본적인 CPU/Memory 외에도 다양한 기준으로 오토스케일링을 설정할 수 있다.

### 4.1 메트릭 유형 (Metric Types)

1. **Resource Metrics:** CPU, Memory 사용량 (Metrics Server 사용).
2. **Custom Metrics:** 파드 내부의 애플리케이션 레벨 메트릭 (예: 초당 요청 수, JVM 힙 사용량 등). 이를 위해서는 `Prometheus Adapter`와 같은 별도 구성이 필요하다.
3. **External Metrics:** 클러스터 외부의 메트릭 (예: AWS SQS 대기열 길이, Pub/Sub 메시지 수).

### 4.2 안정화 윈도우 (Stabilization Window) - Flapping 방지

리소스 사용량이 급격히 변할 때 파드가 생성되고 삭제되는 과정이 반복되는 **플래핑(Flapping)** 현상을 방지하기 위해 쿨다운(Cool-down) 기간을 둔다.

* **scaleDown:** 기본적으로 스케일 다운 결정 후 5분(300초) 동안 대기하여, 짧은 트래픽 감소에 반응하여 파드를 섣불리 줄이지 않도록 한다.
* **scaleUp:** 스케일 업은 즉각적으로 반응하여 서비스 장애를 막는다.

---

## 5. 실전 운영 시 고려사항 (Best Practices)

IT 아키텍트로서 HPA 도입 시 다음 사항들을 반드시 고려해야 한다.

1. **HPA와 VPA 동시 사용 금지:**
* 동일한 메트릭(CPU/Memory)을 기준으로 HPA와 VPA를 같이 사용하면 서로 충돌하여(파드 수를 늘리려는 HPA vs 파드 크기를 키우려는 VPA) 동작이 멈추거나 오작동한다.
* *예외:* VPA를 'Off' 모드(추천만 받음)로 쓰거나, 메트릭을 다르게 설정(HPA는 QPS, VPA는 CPU)하면 가능하다.


2. **Cluster Autoscaler(CA)와의 연동:**
* HPA가 파드를 늘려도 노드(VM)의 자원이 부족하면 파드는 `Pending` 상태에 빠진다.
* 따라서 노드 자체를 자동으로 늘려주는 **Cluster Autoscaler**가 반드시 함께 구성되어야 진정한 오토스케일링이 완성된다.


3. **Cold Start 지연:**
* 트래픽이 폭증할 때, HPA가 이를 감지하고 파드를 생성하여 실제 서비스 투입(Running)까지 수십 초에서 수 분이 걸릴 수 있다.
* 이를 보완하기 위해 예측 가능한 이벤트(마케팅 등) 전에는 미리 `minReplicas`를 수동으로 늘려두는 전략(Scheduled Scaling)이 필요하다.


4. **애플리케이션 특성:**
* Stateless 앱은 HPA 적용이 쉽지만, Stateful 앱(DB 등)은 데이터 동기화 문제로 인해 HPA 적용 시 매우 신중해야 한다.



---

## 6. Summary

| 구분 | HPA (수평 확장) | VPA (수직 확장) | Cluster Autoscaler (노드 확장) |
| --- | --- | --- | --- |
| **대상** | Pod 개수 (Replicas) | Pod 리소스 (Request/Limit) | Node 개수 (VM) |
| **장점** | 무중단 확장, 높은 가용성 | 리소스 최적화, OOM 방지 | 인프라 용량 자동 조절 |
| **단점** | 앱 재시작 불필요하나 초기화 시간 소요 | 파드 재시작 필요 (다운타임 가능성) | 비용 증가, 프로비저닝 시간 소요 |
| **트리거** | CPU, Memory, Custom Metric | OOM, CPU Throttle | Pod Pending 상태 |

Next Step: Cluster Autoscaler(CA)의 동작 원리와 노드 그룹 관리 전략
