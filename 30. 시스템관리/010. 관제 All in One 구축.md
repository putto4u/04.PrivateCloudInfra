# 챕터 8. 매트릭 모니터링 시스템 구축 및 운영 전략

쿠버네티스 환경에서 모니터링 시스템을 구축하는 가장 표준적이고 효율적인 방법은 **헬름(Helm)** 패키지 매니저를 사용하는 것입니다. 수십 개의 YAML 파일을 직접 관리하는 대신, 검증된 커뮤니티 차트인 `kube-prometheus-stack`을 사용하여 프로메테우스, 그라파나, 노드 익스포터, 얼럿매니저를 통합 배포하고 운영하는 방법을 다룹니다.

---

## 1. 헬름(Helm) 기반 설치 준비

실무에서는 개별 구성요소를 따로 설치하지 않고, **`kube-prometheus-stack`**이라는 올인원(All-in-One) 차트를 사용합니다. 이 차트는 모니터링에 필요한 모든 구성요소와 기본 설정, 대시보드 템플릿까지 포함하고 있습니다.

### 1.1. 저장소(Repository) 추가

가장 먼저 헬름 차트 저장소를 로컬 환경에 등록해야 합니다.

```bash
# 프로메테우스 커뮤니티 공식 저장소 추가
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

# 저장소 정보 최신화
helm repo update 

```

### 1.2. 네임스페이스 격리

모니터링 리소스는 시스템의 핵심이므로, 일반 애플리케이션과 섞이지 않도록 별도의 네임스페이스(Namespace)에 격리하여 설치합니다.

```bash
kubectl create namespace monitoring

```

---

## 2. 운영 환경을 고려한 설정 커스터마이징 (values.yaml)

기본 설정(`values.yaml`) 그대로 설치하면 데이터가 보존되지 않거나 리소스 낭비가 발생할 수 있습니다. 운영 환경에 맞게 필수 설정을 재정의해야 합니다.

### 2.1. 주요 설정 포인트

| 설정 항목 | 설명 | 권장 설정 (실습/소규모 운영) |
| --- | --- | --- |
| **retention** | 데이터 보관 주기 | 기본 10일  **3일 ~ 5일** (디스크 용량 절약) |
| **storageClass** | 데이터 영구 저장소 | **gp2 (AWS)** 또는 **local-path (On-premise)** 지정 필수 |
| **service.type** | 그라파나 접근 방식 | ClusterIP  **NodePort** 또는 **LoadBalancer** |
| **adminPassword** | 그라파나 접속 암호 | 초기 비밀번호 설정 (보안 권장) |

### 2.2. 커스텀 설정 파일 작성 (`my-values.yaml`)

```yaml
# my-values.yaml 예시
#스토리지를 쓸 경우 : 프로메테우스 설정을 넣고. 안쓸경우 생략
prometheus:
  prometheusSpec:
    # 데이터 보관 주기 설정 (디스크 용량 고려)
    retention: 3d
    # 영구 저장소(PVC) 설정: 파드가 재시작되어도 데이터 보존
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: gp2  # 환경에 맞는 스토리지 클래스명 입력
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

# 그라파나는 필수(GUI 사용)
grafana:
  # 외부 접속을 위해 NodePort 혹은 LoadBalancer로 변경
  service:
    type: NodePort
    nodePort: 30000  # 고정 포트 사용 시 (선택 사항)
  # 초기 관리자 비밀번호 설정
  adminPassword: "admin"

```

---

## 3. 설치 및 배포 검증

작성한 설정 파일을 기반으로 클러스터에 배포를 진행합니다.

### 3.1. 배포 명령어

```bash
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  -f my-values.yaml \
  --namespace monitoring

또는 스토리지를 안쓸때 
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  -f my-values.yaml \
  --namespace monitoring \
  --set prometheus.prometheusSpec.storageSpec.emptyDir.medium="" \
  --set alertmanager.alertmanagerSpec.storageSpec.emptyDir.medium="" \
  --set grafana.persistence.enabled=false
```

### 3.2. 설치 확인

설치 후 몇 분 뒤, 다음 명령어로 모든 파드(Pod)가 `Running` 상태인지 확인합니다.

```bash
kubectl get pods -n monitoring

```

* **prometheus-0:** 메인 서버 파드 (StatefulSet)
* **grafana-xx:** 시각화 도구 파드
* **node-exporter-xx:** 각 노드마다 1개씩 실행되는 데몬셋

---

## 4. 운영 및 접속 방법

설치가 완료되면 실제로 그라파나 대시보드에 접속하여 데이터가 수집되는지 확인해야 합니다.

### 4.1. 그라파나 대시보드 접속

`my-values.yaml`에서 `NodePort`로 설정했다면, 워커 노드의 IP와 지정된 포트로 접속할 수 있습니다.

* **접속 주소:** `http://<Worker-Node-IP>:30000`
* **계정:** `admin` / `admin` (설정 파일에서 지정한 비밀번호)

### 4.2. 포트 포워딩(Port-Forwarding)을 이용한 임시 접속

`Service` 타입을 변경하지 않았다면, `kubectl`의 포트 포워딩 기능을 사용하여 로컬 PC에서 접속할 수 있습니다.

```bash
# 로컬 8080 포트를 그라파나 80 포트로 연결
kubectl port-forward svc/prometheus-stack-grafana 8080:80 -n monitoring

```

이후 브라우저에서 `localhost:8080`으로 접속합니다.

---

## 5. 아키텍트의 운영 조언 (Operation Tips)

성공적인 모니터링 시스템 운영을 위해 다음 3가지를 반드시 기억해야 합니다.

1. **데이터 영속성(Persistence) 확보:**
* 프로메테우스 파드는 업데이트나 장애로 언제든지 재시작될 수 있습니다. `StorageClass`를 통한 PV(Persistent Volume) 연결 없이는 재시작 시 **모든 과거 데이터가 날아갑니다.** 반드시 PVC 설정을 확인하십시오.


2. **설정 변경은 파일로 관리 (GitOps):**
* 운영 중 설정을 바꿀 때 `kubectl edit` 명령어로 라이브(Live) 상태를 직접 수정하지 마십시오. 반드시 `my-values.yaml` 파일을 수정한 후 `helm upgrade` 명령어로 반영해야 이력을 추적할 수 있습니다.


3. **리소스 모니터링의 모니터링:**
* 프로메테우스 자체도 메모리를 많이 사용하는 애플리케이션입니다. 모니터링 시스템이 죽으면 장애 알림을 받을 수 없으므로, 프로메테우스 파드의 리소스 사용량(Memory Usage)을 주기적으로 체크하고 필요시 `Limit`을 늘려주어야 합니다.



---

# 챕터 9. 매트릭 수집기(Exporter)의 자동 설치와 구성





## 1. 헬름 차트가 자동으로 설치하는 구성요소

`helm install` 명령어를 실행하면, 프로메테우스 서버와 함께 다음 두 가지 핵심 수집기가 자동으로 클러스터에 배포됩니다.

### 1.1. 노드 익스포터 (Node Exporter)

* **역할:** 물리적 하드웨어(노드)의 상태를 측정합니다.
* **수집 데이터:** CPU 사용률, 메모리 잔량, 디스크 I/O, 네트워크 트래픽 등.
* **배포 방식:** **데몬셋(DaemonSet)** 형태로 배포되어, 클러스터에 노드가 추가될 때마다 자동으로 해당 노드에 설치됩니다. (챕터 6에서 다룬 내용이 자동으로 구현됨)

### 1.2. 쿠브 스테이트 매트릭 (Kube State Metrics)

* **역할:** 쿠버네티스 논리적 객체(Object)들의 상태를 측정합니다.
* **수집 데이터:** 파드(Pod)의 상태(Running, Failed 등), 디플로이먼트(Deployment)의 레플리카 개수, 서비스(Service) 상태 등.
* **배포 방식:** **디플로이먼트(Deployment)** 형태로 배포되어, 쿠버네티스 API 서버와 통신하며 데이터를 수집합니다.

---

## 2. 설치 확인 및 검증 (Verification)

실제로 매트릭 수집기가 잘 설치되었는지 확인하는 절차입니다. 터미널에서 다음 명령어를 입력하여 파드 목록을 확인합니다.

```bash
kubectl get pods -n monitoring

```

**[출력 예시 및 분석]**

| 파드 이름 (Prefix) | 상태 | 설명 |
| --- | --- | --- |
| `prometheus-node-exporter-xyz` | **Running** | 각 노드마다 1개씩 실행 중이어야 합니다. (노드가 3개라면 3개가 보임) |
| `kube-state-metrics-abcde` | **Running** | 클러스터 전체 상태를 담당하므로 보통 1개가 실행됩니다. |
| `prometheus-server-0` | **Running** | 수집된 데이터를 저장하는 메인 서버입니다. |
| `grafana-12345` | **Running** | 데이터를 시각화하는 도구입니다. |

> **확인 포인트:**
> 만약 `node-exporter` 파드의 개수가 현재 운영 중인 워커 노드의 개수와 일치한다면, 하드웨어 매트릭 수집 준비가 완벽하게 끝난 것입니다.

---

## 3. 매트릭 데이터 흐름 (Data Flow)

설치가 완료되면 시스템 내부에서는 다음과 같은 흐름으로 매트릭이 수집됩니다.

1. **생성:** `Node Exporter`와 `Kube State Metrics`가 실시간으로 매트릭 데이터를 생성하여 자신의 HTTP 엔드포인트(`/metrics`)에 노출합니다.
2. **수집 (Scrape):** `Prometheus Server`가 설정된 주기(기본 1분)마다 이 엔드포인트에 접속하여 데이터를 긁어갑니다(Pull).
3. **저장:** 긁어온 데이터는 프로메테우스 내부의 시계열 데이터베이스(TSDB)에 저장됩니다.
4. **조회:** 사용자가 `Grafana` 대시보드를 열면, 그라파나가 프로메테우스 DB에 쿼리(PromQL)를 날려 그래프를 그립니다.

---

## 4. 아키텍트의 주의사항 (Architect's Note)

### '매트릭스 서버(Metrics Server)'와의 혼동 주의

쿠버네티스에는 `metrics-server`라는 또 다른 구성요소가 있습니다. 이는 주로 `kubectl top` 명령어를 쓰거나 HPA(오토스케일링)를 할 때 사용됩니다.

* `kube-prometheus-stack`은 모니터링 전용이므로, `metrics-server`와는 별개입니다.
* 하지만 대부분의 헬름 차트는 `metrics-server`가 없어도 프로메테우스만으로 충분히 모니터링이 가능하도록 구성되어 있습니다.
* (참고: `kubectl top` 명령어가 안 먹힌다면 `metrics-server`를 따로 설치해야 하지만, 그라파나 모니터링에는 지장이 없습니다.)

---

**Next Step:** 수집된 데이터가 정상적인지 확인하기 위한 프로메테우스 웹 UI 접속 및 상태 타겟(Target) 확인 실습


---
현재 상황은 이전 설치 시 생성되었던 **StatefulSet의 잔재와 PVC(Persistent Volume Claim)**가 클러스터에 남아있어, 새로 설치한 설정과 충돌하거나 이전의 잘못된 스토리지 설정(`m_stcm`)을 계속 참조하려고 시도하기 때문에 발생합니다.

쿠버네티스에서 Helm을 지워도 `PVC`와 `CustomResourceDefinition(CRD)`은 자동으로 삭제되지 않는 경우가 많습니다. "완전 초기화 후 재설치"를 위한 클리닝 절차를 안내해 드립니다.

---

## 1. 기존 설치 잔해 완전 제거 (Purge)

단순히 `helm uninstall`만으로는 부족합니다. 아래 순서대로 강제 삭제를 진행하십시오.

### Step 1: Helm 삭제

```bash
helm uninstall prometheus-stack -n monitoring

```

### Step 2: 남아있는 리소스 강제 삭제 (핵심)

설정을 '스토리지 없음'으로 바꿨음에도 로그에 스토리지 에러가 보이는 이유는 기존에 생성된 **StatefulSet**과 **PVC**가 삭제되지 않았기 때문입니다.

```bash
# Prometheus와 Alertmanager 관련 StatefulSet 삭제
kubectl delete statefulset -l app.kubernetes.io/instance=prometheus-stack -n monitoring

# 특정 인스턴스 라벨이 붙은 PVC 삭제
kubectl delete pvc -l app.kubernetes.io/instance=prometheus-stack -n monitoring

# PV 상태 확인 후 'Released' 상태인 것 정리
kubectl get pv | grep monitoring | awk '{print $1}' | xargs kubectl delete pv
```

### Step 3: CRD(Custom Resource Definition) 정리

Operator 방식은 CRD를 사용하므로, 이 정보가 꼬이면 재설치 시 이전 설정을 다시 불러올 수 있습니다.

```bash
# Prometheus 관련 CRD 목록 확인 및 삭제
kubectl get crd | grep coreos.com | awk '{print $1}' | xargs kubectl delete crd

```

Validating/Mutating Webhooks 제거: 재설치 시 "Internal error occurred: failed calling webhook" 에러가 발생한다면 이 잔해 때문입니다.

```Bash
kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io -l app.kubernetes.io/instance=prometheus-stack
kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io -l app.kubernetes.io/instance=prometheus-stack
```


4. RBAC 및 설정 잔해 정리 (ClusterRole, ConfigMap)
클러스터 전역 권한 설정과 설정 파일 잔해를 제거합니다.

```Bash
# ClusterRole 및 ClusterRoleBinding 정리
kubectl delete clusterrole -l app.kubernetes.io/instance=prometheus-stack
kubectl delete clusterrolebinding -l app.kubernetes.io/instance=prometheus-stack

# ConfigMap 및 Secret 정리 (monitoring 네임스페이스 기준)
kubectl delete configmap -l app.kubernetes.io/instance=prometheus-stack -n monitoring
kubectl delete secret -l app.kubernetes.io/instance=prometheus-stack -n monitoring
```

5. 실전 팁: 삭제 확인 및 최종 점검
모든 삭제 후 아래 명령어로 남아있는 리소스가 없는지 최종 확인하십시오.

```Bash
# 'prometheus' 또는 'operator' 키워드가 들어간 리소스가 있는지 확인
kubectl get all,crd,pvc,clusterrole,clusterrolebinding,webhook -A | grep -iE "prometheus|operator"

```
---

쿠버네티스 환경에서 컨트롤 플레인(Master) 레벨의 리소스를 정리했더라도, **워커 노드(Worker Node)의 로컬 파일 시스템**에는 컨테이너 런타임이 남긴 잔해들이 여전히 존재할 수 있습니다. 특히 스토리지 문제나 이미지 충돌을 완벽히 해결하려면 노드 단위의 클리닝이 필요합니다.

---

### 1. 워커 노드 내 컨테이너 및 이미지 잔해 정리

프로메테우스 스택은 많은 수의 이미지를 사용합니다. 노드에 남아있는 정지된 컨테이너와 미사용 이미지를 정리하여 노드 환경을 초기화합니다.

* **미사용 리소스 일괄 정리 (Prune):**
* 
```bash
# 각 워커 노드에서 실행
sudo docker system prune -a --volumes  # Docker 환경일 경우
# 또는
sudo crictl rmi --prune                # containerd(K8s 표준) 환경일 경우

```



---

### 2. 로컬 디렉토리 및 마운트 경로 확인 (중요)  


프로메테우스가 `emptyDir`이나 `hostPath`를 사용했다면, `/var/lib/kubelet` 하위 경로에 임시 데이터가 남을 수 있습니다.

* **Kubelet 작업 디렉토리 확인:**
```bash
# 특정 파드의 데이터가 마운트된 채 남아있는지 확인
mount | grep prometheus

# 잔해가 남아있다면 언마운트 시도 (필요 시)
# sudo umount [마운트 경로]

```


* **로그 파일 정리:**
```bash
# 노드 내 축적된 프로메테우스 관련 로그 삭제
sudo rm -rf /var/log/pods/monitoring_prometheus-*
sudo rm -rf /var/log/containers/prometheus-*

```



---

### 3. 노드 커널 설정 (Sysctl) 잔해 점검

일부 모니터링 도구는 노드의 커널 파라미터를 수정하기도 합니다. 하지만 일반적인 `kube-prometheus-stack` 설치 환경에서는 시스템 재부팅이나 `sysctl -p`를 통해 기본값으로 복구가 가능하므로, 수동으로 수정했던 내역이 없다면 큰 문제가 되지 않습니다.

---

### 4. 실전 팁: `Cordon` 및 `Drain` 활용

노드 자체의 꼬임이 의심된다면, 해당 노드에서 실행 중인 워크로드를 비우고 노드를 초기화하는 것이 가장 확실합니다.

1. **노드 비우기:** `kubectl drain [노드이름] --ignore-daemonsets --delete-emptydir-data`
2. **노드 서비스 재시작:** ```bash
sudo systemctl restart kubelet
sudo systemctl restart containerd # 또는 docker
```

```


3. **노드 복구:** `kubectl uncordon [노드이름]`  

---

### 5. 자주 오해하거나 실수하는 부분  

* **이미지 캐시:** 노드에 이전 버전의 프로메테우스 이미지가 캐싱되어 있으면, `imagePullPolicy: IfNotPresent` 설정 시 최신 수정한 설정이 반영되지 않은 예전 이미지를 계속 사용할 수 있습니다. `Always`로 변경하거나 위에서 언급한 `rmi` 명령으로 캐시를 지우는 것이 좋습니다.
* **Zombie Mounts:** 파드는 삭제되었는데 노드 운영체제 레벨에서 마운트 해제가 실패하여 좀비 상태로 남는 경우가 있습니다. 이는 `df -h` 명령 시 `Permission denied` 등의 에러를 유발하므로 반드시 확인이 필요합니다.

---

### 6. 출처 및 참고 문서

* [Kubernetes Documentation: Garbage Collection](https://kubernetes.io/docs/concepts/architecture/garbage-collection/)
* [CRICTL User Guide](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)

Next Step: 컨테이너 런타임(containerd) 캐시 완전 삭제 방법

---

워커 노드 접속 권한이 있으시다면, 각 노드에서 `crictl` 또는 `docker` 명령어를 통해 이미지를 한 번 밀어내고 `kubelet`을 재시작하는 것만으로도 수많은 "알 수 없는 에러"를 해결할 수 있습니다.

> **비용 안내:** 노드 내 로컬 파일 및 이미지 삭제는 별도의 AWS 비용을 발생시키지 않습니다. 오히려 디스크 사용량을 줄여 `EBS` 용량 부족으로 인한 장애를 예방할 수 있습니다.
