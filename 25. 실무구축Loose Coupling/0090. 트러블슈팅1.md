## [실전 트러블슈팅 1] DB 클러스터 구축 시 발생하는 치명적 오류 및 해결

쿠버네티스 환경에서 MySQL Master-Slave 복제(Replication)를 구축할 때, 논리적 설정 오류로 인해 실습이 중단되는 대표적인 사례 3가지를 분석하고 해결합니다. 이 챕터는 프로젝트 시작 전 반드시 숙지해야 할 사전 체크리스트 역할을 합니다.

---

### 1. MySQL 복제 연결 실패 (Server-ID 중복 문제)

가장 빈번하게 발생하는 치명적인 오류입니다. MySQL 복제는 각 서버가 고유한 ID를 가져야만 성립됩니다.

**증상 (Symptom)**
Replica 서버에서 `SHOW SLAVE STATUS\G` 명령을 실행했을 때, 상태가 `Connecting`에서 멈추거나 에러가 발생합니다.

* `Slave_IO_Running`: **Connecting** (또는 No)
* `Last_IO_Error`: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server ids...

**원인 (Root Cause)**
Docker/Kubernetes의 기본 MySQL 이미지는 `server-id`가 `1`로 고정되어 있습니다. 별도 설정 없이 배포하면 Primary와 Replica 모두 ID가 `1`이 되어 충돌이 발생, 복제 연결이 즉시 끊어집니다.

**해결 방법 (Solution)**
배포(Deployment) YAML 파일의 `args` 항목을 수정하여 컨테이너 실행 시 고유한 ID를 부여해야 합니다.

```yaml
# mysql-deploy.yaml 수정 예시

# [Primary 서버]
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    args: ["--server-id=1"]  # Primary는 1번으로 지정

# [Replica-1 서버]
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    args: ["--server-id=11"] # Replica-1은 11번 (Primary와 다르게)

# [Replica-2 서버]
spec:
  containers:
  - name: mysql
    image: mysql:8.0
    args: ["--server-id=12"] # Replica-2는 12번

```

---

### 2. 데이터 동기화 누락 (작업 순서 오류)

복제 설정은 성공했으나, 정작 Primary에 있는 데이터가 Replica에는 없는 현상입니다.

**증상 (Symptom)**

* Primary: `employees` 테이블 데이터 30만 건 존재.
* Replica: `employees` 데이터베이스 자체가 없음 (Empty).
* `Slave_IO_Running`: Yes (연결은 정상).

**원인 (Root Cause)**
**[데이터 복원] → [복제 설정]** 순서로 진행했기 때문입니다.
MySQL의 복제는 **"복제 연결이 맺어진 이후(Binary Log 기록 시점)"**부터 발생하는 변경 사항만 전파합니다. 연결하기 전에 부어넣은 데이터는 Replica가 알 수 없습니다.

**해결 방법 (Solution)**
작업 순서를 논리적으로 재구성해야 합니다.

1. **초기화**: 모든 DB를 빈 상태(Empty)로 시작합니다.
2. **복제 연결**: 빈 DB 상태에서 `CHANGE MASTER TO...` 및 `START SLAVE`를 실행하여 파이프라인을 먼저 연결합니다.
3. **데이터 복원**: 연결이 확인된 후 Primary에 `employees.sql`을 실행(Import)합니다.
* *결과*: 데이터가 입력되는 과정 자체가 로그로 기록되어 Replica로 실시간 전송됩니다.



> **이미 순서가 꼬였다면?**
> Primary에서 `mysqldump`를 떠서 Replica에 수동으로 넣어주어 시점을 맞춘 뒤 다시 복제를 시작해야 합니다.

---

### 3. 외부 접속 불가 (Ingress 및 MetalLB 설정)

브라우저에서 `192.168.0.251`로 접속해도 페이지가 뜨지 않는 네트워크 문제입니다.

**증상 (Symptom)**

* 브라우저: `ERR_CONNECTION_TIMED_OUT` 또는 무한 로딩.
* `kubectl get ingress`: `ADDRESS` 필드가 비어있음.

**원인 (Root Cause)**

1. **MetalLB 미작동**: 온프레미스 환경에서 LoadBalancer IP를 할당해 줄 MetalLB가 없거나, 설정된 IP Pool 범위(`192.168.0.x-y`)에 `251`번이 포함되지 않은 경우입니다.
2. **Ingress Class 누락**: 인그레스 컨트롤러가 여러 개일 경우 `ingressClassName: nginx`를 명시하지 않아 무시되는 경우입니다.

**해결 방법 (Solution)**
MetalLB의 IP Pool 설정을 확인하고, Ingress 매니페스트를 점검합니다.

**1. MetalLB ConfigMap 확인**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.0.250-192.168.0.255  # 251번이 이 범위 안에 있어야 함

```

**2. Ingress 설정 재확인**

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: "nginx" # 컨트롤러 지정 필수
    nginx.ingress.kubernetes.io/rewrite-target: / # 경로 재작성 필수

```

---

Next Step: **back-pvc**
